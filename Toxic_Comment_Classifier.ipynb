{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Toxic Comment Classifier (Responsible AI Edition)\n",
                "\n",
                "This notebook demonstrates a hybrid approach to toxic comment detection. In line with **Responsible AI** principles, we use a combination of transparent rules (keywords) and an interpretable Machine Learning model (Logistic Regression).\n",
                "\n",
                "## ðŸ›¡ï¸ Responsible AI Features:\n",
                "- **Explainability**: Clear rules for blatant toxicity.\n",
                "- **Safety**: Deterministic fallback for high-risk terms.\n",
                "- **Transparency**: Interpretable model coefficients."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Imports\n",
                "First, we install and import the necessary libraries."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import pandas as pd\n",
                "import numpy as np\n",
                "import re\n",
                "import string\n",
                "import nltk\n",
                "import os\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from nltk.corpus import stopwords\n",
                "from nltk.stem import WordNetLemmatizer\n",
                "from sklearn.feature_extraction.text import TfidfVectorizer\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.metrics import confusion_matrix, classification_report\n",
                "\n",
                "# Download NLTK resources\n",
                "nltk.download('stopwords')\n",
                "nltk.download('wordnet')\n",
                "nltk.download('omw-1.4')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Text Preprocessing\n",
                "We define a robust cleaning pipeline to normalize text before it reaches the model."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class TextPreprocessor:\n",
                "    def __init__(self):\n",
                "        self.stop_words = set(stopwords.words('english'))\n",
                "        self.lemmatizer = WordNetLemmatizer()\n",
                "    \n",
                "    def clean_text(self, text):\n",
                "        # Convert to lowercase\n",
                "        text = str(text).lower()\n",
                "        \n",
                "        # Remove punctuation\n",
                "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
                "        \n",
                "        # Remove numbers\n",
                "        text = re.sub(r'\\d+', '', text)\n",
                "        \n",
                "        # Tokenization and Lemmatization\n",
                "        tokens = text.split()\n",
                "        tokens = [self.lemmatizer.lemmatize(word) for word in tokens if word not in self.stop_words]\n",
                "        \n",
                "        return \" \".join(tokens)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Generation\n",
                "For this demo, we use a synthetic dataset containing toxic and non-toxic comments, including potential edge cases."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_sample_data():\n",
                "    data = [\n",
                "        (\"I really love the way you explained this topic. Great job!\", 0),\n",
                "        (\"Can someone help me with this math problem?\", 0),\n",
                "        (\"The weather today is absolutely beautiful.\", 0),\n",
                "        (\"This movie was okay, but I've seen better.\", 0),\n",
                "        (\"I disagree with your point, but I respect your opinion.\", 0),\n",
                "        (\"Welcome to the community! Hope you enjoy your stay.\", 0),\n",
                "        (\"Thank you for sharing this information.\", 0),\n",
                "        (\"How do I install this library on Windows?\", 0),\n",
                "        (\"The cake tastes delicious, thanks for the recipe.\", 0),\n",
                "        (\"Good morning everyone! Have a productive day.\", 0),\n",
                "        (\"You are such a loser, why do you even post here?\", 1),\n",
                "        (\"I hate you and everything you stand for.\", 1),\n",
                "        (\"Shut up, nobody cares about what you think.\", 1),\n",
                "        (\"This is the stupidest thing I have ever read.\", 1),\n",
                "        (\"Go away and never come back, you idiot.\", 1),\n",
                "        (\"You're so ugly and worthless.\", 1),\n",
                "        (\"I'm going to find you and hurt you.\", 1),\n",
                "        (\"You are a terrible person and you should feel bad.\", 1),\n",
                "        (\"Get out of here, we don't want people like you.\", 1),\n",
                "        (\"You're brainless if you think this is correct.\", 1),\n",
                "        (\"I'm going to kill this project if we don't finish it soon.\", 0),\n",
                "        (\"This code is garbage, delete it now.\", 1),\n",
                "        (\"You are freaking awesome!\", 0),\n",
                "        (\"Why are you being so mean?\", 0),\n",
                "        (\"Stop harassing people, it's not cool.\", 0),\n",
                "    ]\n",
                "    return pd.DataFrame(data, columns=['text', 'label'])\n",
                "\n",
                "df = get_sample_data()\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Hybrid Classifier Logic\n",
                "Our classifier checks for high-risk keywords first. If none are found, it uses the ML model's prediction."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class ToxicClassifier:\n",
                "    def __init__(self, keyword_list=None):\n",
                "        self.preprocessor = TextPreprocessor()\n",
                "        self.vectorizer = TfidfVectorizer(max_features=1000)\n",
                "        self.model = LogisticRegression()\n",
                "        self.keyword_list = keyword_list or ['hate', 'ugly', 'stupid', 'loser', 'idiot', 'worthless', 'kill']\n",
                "        self.is_trained = False\n",
                "\n",
                "    def check_keywords(self, text):\n",
                "        text_lower = text.lower()\n",
                "        found_keywords = [word for word in self.keyword_list if word in text_lower]\n",
                "        return len(found_keywords) > 0, found_keywords\n",
                "\n",
                "    def train(self, df):\n",
                "        print(\"Cleaning text...\")\n",
                "        df['cleaned_text'] = df['text'].apply(self.preprocessor.clean_text)\n",
                "        X = self.vectorizer.fit_transform(df['cleaned_text'])\n",
                "        y = df['label']\n",
                "        print(\"Training model...\")\n",
                "        self.model.fit(X, y)\n",
                "        self.is_trained = True\n",
                "\n",
                "    def predict(self, text):\n",
                "        keyword_flag, found = self.check_keywords(text)\n",
                "        cleaned = self.preprocessor.clean_text(text)\n",
                "        if not cleaned:\n",
                "             ml_prob, ml_pred = 0.0, 0\n",
                "        else:\n",
                "            X_vec = self.vectorizer.transform([cleaned])\n",
                "            ml_prob = self.model.predict_proba(X_vec)[0][1]\n",
                "            ml_pred = self.model.predict(X_vec)[0]\n",
                "        \n",
                "        final_flag = 1 if (keyword_flag or ml_pred == 1) else 0\n",
                "        return {\n",
                "            'text': text,\n",
                "            'keyword_flag': int(keyword_flag),\n",
                "            'found_keywords': found,\n",
                "            'ml_probability': float(ml_prob),\n",
                "            'final_prediction': final_flag\n",
                "        }"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Training and Evaluation\n",
                "We train the model and visualize its performance using a Confusion Matrix."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "classifier = ToxicClassifier()\n",
                "classifier.train(df)\n",
                "\n",
                "# Evaluation\n",
                "y_true = df['label']\n",
                "y_pred = classifier.model.predict(classifier.vectorizer.transform(df['text'].apply(classifier.preprocessor.clean_text)))\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_true, y_pred, target_names=['Non-Toxic', 'Toxic']))\n",
                "\n",
                "# Plot Confusion Matrix\n",
                "plt.figure(figsize=(6, 4))\n",
                "sns.heatmap(confusion_matrix(y_true, y_pred), annot=True, fmt='d', cmap='Blues')\n",
                "plt.title('Confusion Matrix')\n",
                "plt.ylabel('Actual')\n",
                "plt.xlabel('Predicted')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Interactive Testing\n",
                "Test the hybrid logic with your own comments!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "test_comments = [\n",
                "    \"You are an absolute genius!\",\n",
                "    \"This is stupid code.\",\n",
                "    \"I'm going to kill this bug.\",\n",
                "    \"You are a worthless loser.\"\n",
                "]\n",
                "\n",
                "for comment in test_comments:\n",
                "    res = classifier.predict(comment)\n",
                "    status = \"ðŸš© TOXIC\" if res['final_prediction'] == 1 else \"âœ… SAFE\"\n",
                "    print(f\"Comment: {comment}\")\n",
                "    print(f\"Result: {status} (ML Prob: {res['ml_probability']:.2f}, Keywords: {res['found_keywords']})\\n\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}